{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Text Generation  model using LSTM and TensorFlow\n",
    "\n",
    "Welcome to this notebook on text generation using LSTM and TensorFlow. This notebook offers a straightforward approach to understanding and implementing a simple LSTM-based text generation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to this comprehensive guide! In this notebook, you will learn how to build a text generation model using LSTM models with TensorFlow and Keras. The focus is to create a model capable of predicting and generating the next word in a sentence.\n",
    "\n",
    "#### What You'll Learn\n",
    "In this notebook, you will:\n",
    "- **Prepare** the dataset for training.\n",
    "- **Design** the LSTM model architecture.\n",
    "- **Train** the model.\n",
    "- **Evaluate** its performance.\n",
    "\n",
    "#### Why Text Generation?\n",
    "Text generation is an exciting field with numerous applications, from creative writing to enhancing interactive systems. By the end of this notebook, you'll have a functional model and a deeper understanding of how text generation works.\n",
    "\n",
    "Let’s get started and build something innovative!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "In this section, we will import the essential libraries required for building and training our text generation model. These libraries include TensorFlow and Keras for constructing and training the LSTM model, as well as libraries for data manipulation and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt      \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import re                                  \n",
    "import string         \n",
    "import nltk   \n",
    "from nltk.tokenize import word_tokenize                     \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from keras.layers import Attention\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "\n",
    "After setting up our libraries, our next step is to load the dataset we need. The quality and structure of our data play a crucial role in how well our model performs. We’ll be utilizing the datasets library to fetch the \"bookcorpus\" dataset. We’ll use the first 50% of the training split to keep it efficient and manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "dataset = load_dataset(\"bookcorpus\",split='train[:50%]') \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "Now that we've loaded our dataset, the next step is to convert it to a pandas DataFrame for easier manipulation. We'll extract the text column which contains the data we need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=dataset.to_pandas()\n",
    "ds=ds['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing \n",
    "\n",
    "Once our data is loaded and properly converted, we move on to the next critical step: text processing. This involves the clean-up and tokenization of the text to ensure it is ready for analysis. The `text_processing` function accomplishes this by converting text to lowercase, removing links and non-alphanumeric characters, and tokenizing the sentence while filtering out unnecessary stopwords and empty tokens. Here's the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text_example):\n",
    "    \n",
    "    # convert all letters to lower case\n",
    "    example = text_example.lower()\n",
    "\n",
    "    # Remove links\n",
    "    example = re.sub(r'http\\S+|www.\\S+|@|️#|', '', example)\n",
    "\n",
    "    # Remove other non-alphanumeric characters \n",
    "    example = re.sub(r'[^a-zA-Z0-9 .]', ' ', example)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence_tokens = word_tokenize(example)\n",
    "    \n",
    "    sentence_tokens = [token for token in sentence_tokens if token.strip() and token not in stopwords and token !='#']\n",
    "        \n",
    "    sentence_tokens = [token for token in sentence_tokens if token and token != '️']\n",
    "    \n",
    "#     sentence_tokens =['<s>']+sentence_tokens +['</s>']\n",
    "    \n",
    "    return sentence_tokens\n",
    "\n",
    "\n",
    "print('original text: ',ds[0])\n",
    "print('processing text : ',text_processing(ds[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Text Processing to the Dataset\n",
    "\n",
    "Now that we have defined our text processing function, we can apply it to our dataset to ensure all text data is clean and ready for analysis. The following code applies the `text_processing` function to each element in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.apply(text_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Text Processing Results\n",
    "\n",
    "After applying the text processing function to the dataset, it is essential to ensure that our preprocessing steps were effective. One way to do this is by verifying if specific unwanted tokens, like the `#` symbol, have been successfully removed. The following code checks for the presence of such tokens in the tokenized sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token(token, tokenized_sentences):\n",
    "    for sentence in tokenized_sentences:\n",
    "        if token in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "token = '#'\n",
    "exists = check_token(token, ds)\n",
    "\n",
    "print(f\"Does the token '{token}' exist in the tokenized sentences? {exists}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(ds)\n",
    "total_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for sentence in ds for word in sentence]\n",
    "print(len(Counter(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_sequences(sentences,total_words):\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = Tokenizer()\n",
    "    \n",
    "#     # Flatten the list of sentences and fit the tokenizer\n",
    "#     all_words = [word for sentence in sentences for word in sentence]\n",
    "#     tokenizer.fit_on_texts(all_words)\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            # Convert the sentence up to i+1 to a sequence\n",
    "            sequence = tokenizer.texts_to_sequences([sentence[:i+1]])[0]\n",
    "            sequences.append(sequence)\n",
    "            \n",
    "    max_len= 5 # find_optimal_maxlen(sequences)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    sequences = pad_sequences(sequences,maxlen=max_len, padding='pre')\n",
    "    \n",
    "    # Split the padded sequences into inputs and targets\n",
    "    inputs = sequences[:, :-1]\n",
    "    targets = sequences[:, -1]\n",
    "    targets = to_categorical(targets, num_classes=total_words)\n",
    "\n",
    "    \n",
    "    return inputs, targets #,tokenizer\n",
    "\n",
    "inputs, targets = generate_tokenized_sequences(ds,total_words)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few inputs and targets\n",
    "for i in range(min(5, len(inputs))):\n",
    "    input_words = [tokenizer.index_word[idx] if idx in tokenizer.index_word else '<OOV>' for idx in inputs[i]]\n",
    "    target_word = tokenizer.index_word[targets[i].argmax()] if targets[i].argmax() in tokenizer.index_word else '<OOV>'\n",
    "    \n",
    "    print(f\"Input {i+1}: {inputs[i]} ({input_words})\")\n",
    "    print(f\"Target {i+1}: {targets[i]} ({target_word})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequences_length(sequences):\n",
    "    # Get the length of the first sequence\n",
    "    first_sequence_length = len(sequences[0])\n",
    "\n",
    "    # Check if all sequences have the same length\n",
    "    return all(len(sequence) == first_sequence_length for sequence in sequences)\n",
    "\n",
    "print(check_sequences_length(inputs))  # Outputs: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =len(tokenizer.word_index)+1\n",
    "maxlen=len(inputs[0])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=False),\n",
    "    Embedding(total_words, 300),\n",
    "    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=128, dropout=0.2)),\n",
    "    Dense(units=128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "history = model.fit(inputs, targets,batch_size=batch_size,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(inputs, targets, batch_size):\n",
    "    num_samples = len(inputs)\n",
    "    while True:  # Loop forever, so the generator never runs out of data\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            batch_targets = targets[i:i+batch_size]\n",
    "            yield batch_inputs, batch_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 2860 # batch size\n",
    "generator = data_generator(inputs, targets, batch_size)\n",
    "\n",
    "steps_per_epoch = len(inputs) // batch_size  # Number of batches per epoch\n",
    "\n",
    "history = model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')  # saves the model in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('/kaggle/input/model2tgh5/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Text Generation  model using LSTM and TensorFlow\n",
    "\n",
    "Welcome to this notebook on text generation using LSTM and TensorFlow. This notebook offers a straightforward approach to understanding and implementing a simple LSTM-based text generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt      \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import re                                  \n",
    "import string         \n",
    "import nltk   \n",
    "from nltk.tokenize import word_tokenize                     \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from keras.layers import Attention\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "dataset = load_dataset(\"bookcorpus\",split='train[:50%]') \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=dataset.to_pandas()\n",
    "ds=ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text_example):\n",
    "    \n",
    "    # convert all letters to lower case\n",
    "    example = text_example.lower()\n",
    "\n",
    "    # Remove links\n",
    "    example = re.sub(r'http\\S+|www.\\S+|@|️#|', '', example)\n",
    "\n",
    "    # Remove other non-alphanumeric characters \n",
    "    example = re.sub(r'[^a-zA-Z0-9 .]', ' ', example)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence_tokens = word_tokenize(example)\n",
    "    \n",
    "    sentence_tokens = [token for token in sentence_tokens if token.strip() and token not in stopwords and token !='#']\n",
    "        \n",
    "    sentence_tokens = [token for token in sentence_tokens if token and token != '️']\n",
    "    \n",
    "#     sentence_tokens =['<s>']+sentence_tokens +['</s>']\n",
    "    \n",
    "    return sentence_tokens\n",
    "\n",
    "\n",
    "print('original text: ',ds[0])\n",
    "print('processing text : ',text_processing(ds[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token(token, tokenized_sentences):\n",
    "    for sentence in tokenized_sentences:\n",
    "        if token in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "token = '#'\n",
    "exists = check_token(token, ds)\n",
    "\n",
    "print(f\"Does the token '{token}' exist in the tokenized sentences? {exists}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(ds)\n",
    "total_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for sentence in ds for word in sentence]\n",
    "print(len(Counter(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_sequences(sentences,total_words):\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = Tokenizer()\n",
    "    \n",
    "#     # Flatten the list of sentences and fit the tokenizer\n",
    "#     all_words = [word for sentence in sentences for word in sentence]\n",
    "#     tokenizer.fit_on_texts(all_words)\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            # Convert the sentence up to i+1 to a sequence\n",
    "            sequence = tokenizer.texts_to_sequences([sentence[:i+1]])[0]\n",
    "            sequences.append(sequence)\n",
    "            \n",
    "    max_len= 5 # find_optimal_maxlen(sequences)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    sequences = pad_sequences(sequences,maxlen=max_len, padding='pre')\n",
    "    \n",
    "    # Split the padded sequences into inputs and targets\n",
    "    inputs = sequences[:, :-1]\n",
    "    targets = sequences[:, -1]\n",
    "    targets = to_categorical(targets, num_classes=total_words)\n",
    "\n",
    "    \n",
    "    return inputs, targets #,tokenizer\n",
    "\n",
    "inputs, targets = generate_tokenized_sequences(ds,total_words)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few inputs and targets\n",
    "for i in range(min(5, len(inputs))):\n",
    "    input_words = [tokenizer.index_word[idx] if idx in tokenizer.index_word else '<OOV>' for idx in inputs[i]]\n",
    "    target_word = tokenizer.index_word[targets[i].argmax()] if targets[i].argmax() in tokenizer.index_word else '<OOV>'\n",
    "    \n",
    "    print(f\"Input {i+1}: {inputs[i]} ({input_words})\")\n",
    "    print(f\"Target {i+1}: {targets[i]} ({target_word})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequences_length(sequences):\n",
    "    # Get the length of the first sequence\n",
    "    first_sequence_length = len(sequences[0])\n",
    "\n",
    "    # Check if all sequences have the same length\n",
    "    return all(len(sequence) == first_sequence_length for sequence in sequences)\n",
    "\n",
    "print(check_sequences_length(inputs))  # Outputs: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =len(tokenizer.word_index)+1\n",
    "maxlen=len(inputs[0])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=False),\n",
    "    Embedding(total_words, 300),\n",
    "    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=128, dropout=0.2)),\n",
    "    Dense(units=128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "history = model.fit(inputs, targets,batch_size=batch_size,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(inputs, targets, batch_size):\n",
    "    num_samples = len(inputs)\n",
    "    while True:  # Loop forever, so the generator never runs out of data\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            batch_targets = targets[i:i+batch_size]\n",
    "            yield batch_inputs, batch_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 2860 # batch size\n",
    "generator = data_generator(inputs, targets, batch_size)\n",
    "\n",
    "steps_per_epoch = len(inputs) // batch_size  # Number of batches per epoch\n",
    "\n",
    "history = model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

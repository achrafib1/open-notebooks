{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Face Detection model with YOLOv7: A Comprehensive Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through the process of building a face detection model using YOLOv7. Each code block will be explained to ensure you understand the steps involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn import preprocessing\n",
    "import shutil\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries are essential for data manipulation (`pandas`, `numpy`), machine learning (`tensorflow`, `sklearn`), image processing (`cv2`, `PIL.Image`), and visualization (`matplotlib`, `IPython.display`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up YOLOv7\n",
    "In this step, we will download the YOLOv7 repository and install the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/WongKinYiu/yolov7 # Downloading YOLOv7 repository\n",
    "%cd yolov7\n",
    "!pip3 install -qr requirements.txt # Installing requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `git clone` command is used to clone the YOLOv7 repository from GitHub. The `%cd` command changes the current directory to the cloned repository. Finally, the `pip3 install -qr requirements.txt` command installs the necessary requirements for YOLOv7 from the `requirements.txt` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Pretrained Weights\n",
    "In this step, we will download the pretrained weights for YOLOv7. These weights have been trained on a large dataset and can help improve the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wget` command is a free utility for non-interactive download of files from the web. Here, we are using it to download the pretrained weights for YOLOv7 from the given URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "We start by loading the 'wider_face' dataset and converting it to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('wider_face',split='train')\n",
    "df=ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset\n",
    "Let's print the index of our dataframe and the bounding box coordinates of the third face in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index)\n",
    "print(df.faces.iloc[3]['bbox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Visualizing an Image\n",
    "Next, we load an image from our dataset, convert its color from BGR to RGB (as OpenCV loads images in BGR format), and print its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=df.image.iloc[0]['path']\n",
    "img = cv2.imread(path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Bounding Boxes\n",
    "In the following block, we draw bounding boxes on the faces detected in our image and display it to as part of our exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bbox in df.faces.iloc[0]['bbox']:\n",
    "  x1, y1, x2, y2 = bbox\n",
    "  cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code draws rectangles around the detected faces in the image using the coordinates provided in the 'bbox' field of our dataframe. The rectangles are drawn in red color (255, 0, 0) with a thickness of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding\n",
    "In this step, we start by defining our labels. In this case, we only have one label: 'face'. We then use LabelEncoder to transform this label into a format that our model can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['face']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method is used to fit the LabelEncoder to our labels. This will assign an integer to each label, which can be useful when working with algorithms that require numerical input. The `classes_` attribute of the fitted LabelEncoder will give us the classes that the LabelEncoder has learned, which we print out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting COCO Bounding Boxes to YOLO Format\n",
    "Next, we define a function to convert bounding boxes from COCO format to YOLO format. This is necessary because YOLOv7 requires bounding boxes to be in a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_to_yolo(coco_boxes, img_width, img_height,le):\n",
    "    yolo_boxes = []\n",
    "    labelcode = le.transform(['face'])[0]\n",
    "    for box in coco_boxes:\n",
    "        x_min, y_min, width, height = box\n",
    "        x_center = x_min + width / 2.0\n",
    "        y_center = y_min + height / 2.0\n",
    "        x_center_norm = x_center / img_width\n",
    "        y_center_norm = y_center / img_height\n",
    "        width_norm = width / img_width\n",
    "        height_norm = height / img_height\n",
    "        yolo_boxes.append([labelcode,x_center_norm, y_center_norm, width_norm, height_norm])\n",
    "    return yolo_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first transform the label 'face' into a numerical code using the LabelEncoder we fitted earlier. Then, for each bounding box in COCO format, we calculate the center of the box and normalize the coordinates and dimensions by the image width and height. These normalized values are then appended to our list of YOLO formatted bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Dataset\n",
    "Next, we define a function to save our dataset in a format suitable for training our YOLOv7 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savedatasetfhs(row):\n",
    "  a=row.name\n",
    "  if(a< 6500):\n",
    "    path=row[0]['path']\n",
    "    img=cv2.imread(path)\n",
    "    image=tf.keras.preprocessing.image.array_to_img(load_image(row[0]['path']))\n",
    "    image.save('/kaggle/working/images/image'+str(a)+'.jpeg')\n",
    "    np.savetxt('/kaggle/working/images/image'+str(a)+'.txt', coco_to_yolo(row[1]['bbox'],img.shape[1],img.shape[0],le))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first check if the row number is less than 6500. If it is, we load the image from the path provided in the row, convert it to an image object using TensorFlow's `array_to_img` function, and save it as a JPEG file in the '/kaggle/working/images/' directory. We also convert the bounding box coordinates from COCO format to YOLO format using our previously defined `coco_to_yolo` function, and save these coordinates as a text file in the same directory.\n",
    "\n",
    "Finally, we apply this function to each row in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(savedatasetfhs,axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code applies the `savedatasetfhs` function to each row (or column) in our dataframe, effectively saving our entire dataset in a format suitable for training our YOLOv7 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "In this step, we define a function to split our dataset into training and validation sets. This is important for evaluating the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(folder_path, train_size=0.8):\n",
    "    # Create the output directories\n",
    "    os.makedirs(os.path.join('split', 'images', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join('split', 'images', 'val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join('split', 'labels', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join('split', 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "    # Get the list of image and label files in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpeg')]\n",
    "    label_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "    # Sort the lists of files\n",
    "    image_files.sort()\n",
    "    label_files.sort()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(image_files, label_files, train_size=train_size)\n",
    "\n",
    "    # Copy the training images and labels to the output directories\n",
    "    for image_file, label_file in zip(train_images, train_labels):\n",
    "        shutil.copy(os.path.join(folder_path, image_file), os.path.join('split', 'images', 'train', image_file))\n",
    "        shutil.copy(os.path.join(folder_path, label_file), os.path.join('split', 'labels', 'train', label_file))\n",
    "\n",
    "    # Copy the validation images and labels to the output directories\n",
    "    for image_file, label_file in zip(val_images, val_labels):\n",
    "        shutil.copy(os.path.join(folder_path, image_file), os.path.join('split', 'images', 'val', image_file))\n",
    "        shutil.copy(os.path.join(folder_path, label_file), os.path.join('split', 'labels', 'val', label_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates separate directories for training and validation images and labels. It then splits the data into training and validation sets and copies the corresponding images and labels into the appropriate directories.\n",
    "\n",
    "Finally, we call this function to split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/kaggle/working/images'\n",
    "split_data(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the Data Split\n",
    "In this step, we verify that our data has been correctly split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = \"split/images/train\"\n",
    "train_label_path = \"split/labels/train\"\n",
    "\n",
    "val_img_path = \"split/images/val\"\n",
    "val_label_path = \"split/labels/val\"\n",
    "\n",
    "# Get the list of image and label files in the train and validation folders\n",
    "image_files_train=[f for f in os.listdir(train_img_path) if f.endswith('.jpeg')]\n",
    "label_files_train=[f for f in os.listdir(train_label_path) if f.endswith('.txt')]\n",
    "image_files_val=[f for f in os.listdir(val_img_path) if f.endswith('.jpeg')]\n",
    "label_files_val=[f for f in os.listdir(val_label_path) if f.endswith('.txt')]\n",
    "\n",
    "# Sort the lists of files\n",
    "image_files_train.sort()\n",
    "label_files_train.sort()\n",
    "image_files_val.sort()\n",
    "label_files_val.sort()\n",
    "\n",
    "# Remove the file extensions\n",
    "for f in range(len(image_files_train)):\n",
    "  image_files_train[f]=f'{image_files_train[f].split(\".\")[0]}'\n",
    "for f in range(len(label_files_train)):\n",
    "  label_files_train[f]=f'{label_files_train[f].split(\".\")[0]}'\n",
    "for f in range(len(image_files_val)):\n",
    "  image_files_val[f]=f'{image_files_val[f].split(\".\")[0]}'\n",
    "for f in range(len(label_files_val)):\n",
    "  label_files_val[f]=f'{label_files_val[f].split(\".\")[0]}'\n",
    "\n",
    "# Check if the number of images matches the number of labels and if the filenames match\n",
    "if(len(image_files_train)==len(label_files_train) and image_files_train== label_files_train):\n",
    "  print('Successful train split ')\n",
    "  print('Number of train images: '+str(len(image_files_train)))\n",
    "  print('Number of train labels: '+str(len(label_files_train)))\n",
    "else:\n",
    "  print('Unsuccessful train split')\n",
    "if(len(image_files_val)==len(label_files_val) and image_files_val== label_files_val):\n",
    "  print('Successful validation split ')\n",
    "  print('Number of validation images: '+str(len(image_files_val)))\n",
    "  print('Number of validation labels: '+str(len(label_files_val)))\n",
    "else:\n",
    "  print('Unsuccessful validation split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code verifies that the number of images matches the number of labels in both the training and validation sets, and that the filenames match. If the split was successful, it prints the number of images and labels in each set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a YAML Configuration File\n",
    "In this step, we create a YAML configuration file that specifies the paths to the training and validation sets, the number of classes, and the class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo -e \"train: /kaggle/working/yolov7/split/images/train\\nval: /kaggle/working/yolov7/split/images/val\\n\\nnc: 1\\nnames: ['face']\" >> rbcdet.yaml\n",
    "! cat 'rbcdet.yaml'\n",
    "shutil.copyfile('/kaggle/working/yolov7/rbcdet.yaml', '/kaggle/working/yolov7/data/rbcdet.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The echo command is used to create the YAML file with the necessary configurations. The -e option enables interpretation of backslash escapes. The >> operator appends the output to the rbcdet.yaml file. The cat command is used to display the contents of the rbcdet.yaml file. Finally, the shutil.copyfile function is used to copy the rbcdet.yaml file to the /kaggle/working/yolov7/data/ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Configuration File\n",
    "In this step, we update the Intersection Over Union (IOU) threshold in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/iou_t: 0.2/iou_t: 0.6/'  /kaggle/working/yolov7/data/hyp.scratch.p5.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sed` command is used here to edit the `hyp.scratch.p5.yaml` file in-place (`-i`). The `s/iou_t: 0.2/iou_t: 0.6/` argument tells `sed` to replace (`s`) the first occurrence of `iou_t: 0.2` with `iou_t: 0.6` in each line of the file. This effectively updates the IOU threshold from 0.2 to 0.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model with Updated Configuration\n",
    "In this step, we train our YOLOv7 model using the updated configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --batch 16 --epochs 25 --data /kaggle/working/yolov7/data/rbcdet.yaml --weights '/kaggle/working/yolov7/yolov7_training.pt'   --img=640 --freeze 5 --hyp data/hyp.scratch.p5.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train.py` script is used to train the YOLOv7 model. The `--batch 16` option sets the batch size to 16. The `--epochs 25` option sets the number of epochs to 25. The `--data /kaggle/working/yolov7/data/rbcdet.yaml` option specifies the path to the YAML configuration file. The `--weights '/kaggle/working/yolov7/yolov7_training.pt'` option specifies the path to the weights of the trained model. The `--img=640` option sets the size of the images to 640 pixels. The `--freeze 5` option freezes the first 5 layers of the model during training. The `--hyp data/hyp.scratch.p5.yaml` option specifies the path to the hyperparameters configuration file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running the Model on Test Images\n",
    "In this step, we use the `detect.py` script provided in the YOLOv7 repository to run our trained model on the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h!python detect.py --weights /kaggle/working/yolov7/runs/train/exp4/weights/best.pt --img 256 --conf 0.40 --source /kaggle/input/testdetect/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect.py` script is used to run the YOLOv7 model on images. The `--weights /kaggle/working/yolov7/runs/train/exp4/weights/best.pt` option specifies the path to the weights of the trained model. The `--img 256` option sets the size of the images to 256 pixels. The `--conf 0.40` option sets the confidence threshold for detections to 0.40. The `--source /kaggle/input/testdetect/` option specifies the directory containing the test images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Displaying Inference on Test Images\n",
    "In this step, we display the inference results on all test images. We set a limit to the maximum number of images to print.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "limit = 10000 # max images to print\n",
    "for imageName in glob.glob('/kaggle/working/yolov7/runs/detect/exp8/*.jpeg'): #assuming JPG\n",
    "    if i < limit:\n",
    "      display(Image(filename=imageName))\n",
    "      print(\"\\n\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code loops through all the JPEG images in the specified directory. For each image, if the number of images displayed so far is less than the limit, it displays the image and prints a newline. The `glob.glob` function is used to get the list of all JPEG images in the directory. The `display` function from the `IPython.display` module is used to display the images, and the `Image` function is used to load the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
